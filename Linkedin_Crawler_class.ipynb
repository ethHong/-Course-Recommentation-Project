{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from urllib import request\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import os\n",
    "from fake_useragent import UserAgent\n",
    "from tqdm.notebook import trange\n",
    "import time\n",
    "from ast import literal_eval\n",
    "\n",
    "class JDcrawler_recommender():\n",
    "\n",
    "    def __init__(self, name, driverpath, driver, options):\n",
    "        print(\"크롤러 초기 설정중...\")\n",
    "        self.ID = input(\"ID:\")\n",
    "        self.PASS = input(\"PASS: \")\n",
    "        self.driverpath = driverpath\n",
    "        self.driver = driver\n",
    "        self.options = options\n",
    "        self.name = name\n",
    "\n",
    "        print(\"안녕하세요 {}님, 당신의 직무, 커리어 관심도에 따른 수업 추천을 해드리겠습니다.\".format(name))\n",
    "        print(\"1. 먼저 관심있는 기업 / 직무 / 산업분야를 질문에 따라 순서대로 입력하세요. '무관' 한 분야는 스킵하셔도 됩니다.\")\n",
    "        print(\"2. 이후 입력한 내용에 따라 10개의 Job Description 이 노출됩니다. 이중 마음에 드는 3개를 고르세요!\")\n",
    "        print(\"3. 이 결과를 바탕으로 수업을 추천해드립니다\")\n",
    "        print(\"**모든 검색어는 영문으로 입력하셔야 합니다!**\")\n",
    "\n",
    "        self.topicnum = input(\"초기 토픽 갯수 설정 (25, 40, 50중 입력): \")\n",
    "        self.industry = input(\"관심있는 산업을 입력해주세요 (없으면 Enter 입력)\")\n",
    "        self.company = input(\"관심있는 기업을 입력해주세요 (없으면 Enter 입력)\")\n",
    "        self.job = input(\"관심있는 직무 입력해주세요(없으면 Enter 입력)\")\n",
    "\n",
    "        self.keyword = \"-\".join((self.industry + \" \" + self.company + \" \" + self.job).split()).lower()\n",
    "\n",
    "    def load_processed(self):\n",
    "        processed = pd.read_csv(\n",
    "            os.getcwd() + \"/train_dataset\" + \"/processed_courses_data_{}topic.csv\".format(self.topicnum))\n",
    "        return processed\n",
    "\n",
    "    def mock_user_agent(self):\n",
    "        ua = UserAgent()\n",
    "        working = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.1 Safari/605.1.15\"\n",
    "        working_tail = \"(\" + working.split(\"(\")[-1]\n",
    "        random_head = ua.random.split(\"(\")[0] + \"(\" + ua.random.split(\"(\")[1]\n",
    "        return random_head + working_tail\n",
    "\n",
    "    def check_http_error(self):\n",
    "        return \"HTTP ERROR 429\" in self.driver.page_source\n",
    "\n",
    "    def login_linkedin(self):\n",
    "\n",
    "        self.options.add_experimental_option('excludeSwitches', ['enable-automation'])\n",
    "\n",
    "        userAgent = self.mock_user_agent()\n",
    "        self.options.add_argument(f'user-agent={userAgent}')\n",
    "        driverpath = os.getcwd() + \"/chromedriver\"\n",
    "        wait = WebDriverWait(self.driver, 10)\n",
    "\n",
    "        url = \"https://www.linkedin.com/\"\n",
    "        self.driver.get(url)\n",
    "\n",
    "        # driver.find_element_by_xpath('/html/body/div/main/p/a').click()\n",
    "        try:\n",
    "            elem = self.driver.find_element_by_xpath('//*[@id=\"session_key\"]')\n",
    "            elem.send_keys(self.ID)\n",
    "            elem = self.driver.find_element_by_xpath('//*[@id=\"session_password\"]')\n",
    "            elem.send_keys(self.PASS)\n",
    "\n",
    "            self.driver.find_element_by_xpath('/html/body/main/section[1]/div[2]/form/button').click()\n",
    "        except:\n",
    "            if self.check_http_error() == True:\n",
    "                print(\"Take a break...for 3 minuits...\")\n",
    "                time.sleep(180)\n",
    "\n",
    "                elem = self.driver.find_element_by_xpath('//*[@id=\"session_key\"]')\n",
    "                elem.send_keys(self.ID)\n",
    "                elem = self.driver.find_element_by_xpath('//*[@id=\"session_password\"]')\n",
    "                elem.send_keys(self.PASS)\n",
    "\n",
    "                self.driver.find_element_by_xpath('/html/body/main/section[1]/div[2]/form/button').click()\n",
    "\n",
    "    def refresh_link(self, continue_link):\n",
    "        userAgent = self.mock_user_agent()\n",
    "        self.options.add_argument(f'user-agent={userAgent}')\n",
    "        self.driverpath = os.getcwd() + \"/chromedriver\"\n",
    "        self.driver = webdriver.Chrome(self.driverpath, chrome_options=self.options)\n",
    "\n",
    "        self.login_linkedin()\n",
    "        self.driver.get(continue_link)\n",
    "\n",
    "    def refine(self, c):\n",
    "        c_ref = \"-\".join(c.split(\" \")).lower()\n",
    "        return c_ref\n",
    "\n",
    "    def refresh_source_pages(self):\n",
    "        time.sleep(1)\n",
    "        html = self.driver.page_source\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        try:\n",
    "            p = soup.find(\"ul\", {\"class\": \"artdeco-pagination__pages artdeco-pagination__pages--number\"}).find_all(\"li\")\n",
    "        except:\n",
    "            self.driver.get(self.driver.current_url)\n",
    "            time.sleep(5)\n",
    "            p = soup.find(\"ul\", {\"class\": \"artdeco-pagination__pages artdeco-pagination__pages--number\"}).find_all(\"li\")\n",
    "        return [p, soup]\n",
    "\n",
    "    def crawl_jd(self):\n",
    "        time.sleep(1)\n",
    "        soup = self.refresh_source_pages()[1]\n",
    "\n",
    "        potision = []\n",
    "        job_details = []\n",
    "\n",
    "        jobs = soup.find_all(\"li\", {\"class\": \"jobs-search-results__list-item occludable-update p0 relative ember-view\"})\n",
    "        jobs_id = [i[\"id\"] for i in jobs]\n",
    "\n",
    "        for i in jobs_id:\n",
    "            self.driver.find_element_by_xpath('//*[@id=\"{}\"]'.format(i)).click()\n",
    "\n",
    "            self.driver.implicitly_wait(10)\n",
    "            # refresh page source\n",
    "            soup = self.refresh_source_pages()[1]\n",
    "\n",
    "            self.driver.implicitly_wait(10)\n",
    "\n",
    "            Position = soup.find(\"h2\",\n",
    "                                 {\"class\": \"jobs-details-top-card__job-title t-20 t-black t-normal\"}).text.rstrip()\n",
    "\n",
    "            Job_Details = soup.find(\"div\", {\"id\": \"job-details\"}).text.strip()\n",
    "            potision.append(Position)\n",
    "            job_details.append(Job_Details)\n",
    "\n",
    "        return pd.DataFrame({\"Position\": potision, \"Job_Details\": job_details})\n",
    "\n",
    "    def crawl_job_description(self, starting_page, how_many, total_page, start_url):\n",
    "        if how_many > total_page:\n",
    "            how_many = total_page\n",
    "\n",
    "        self.driver.get(start_url)\n",
    "        self.driver.implicitly_wait(10)\n",
    "        time.sleep(1)\n",
    "        pages = self.refresh_source_pages()[0]\n",
    "        soup = self.refresh_source_pages()[1]\n",
    "\n",
    "        current = starting_page\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        for i in trange(starting_page - 1, how_many + starting_page - 1):\n",
    "\n",
    "            print(\"Crawling {} out of {} pages...\".format(current, total_page))\n",
    "\n",
    "            pages_meta = [j.text.strip().split()[0] for j in pages]\n",
    "\n",
    "            # Do Crawling#\n",
    "\n",
    "            crawed_page = self.crawl_jd()\n",
    "            df = pd.concat([df, crawed_page])\n",
    "            current = current + 1\n",
    "\n",
    "            if current > total_page:\n",
    "                break  # Don't move page if it's last page\n",
    "            # Move page\n",
    "\n",
    "            try:\n",
    "                index_of_next_page = pages_meta.index(str(i + 2))\n",
    "            except ValueError:\n",
    "                index_of_next_page = len(pages_meta) - 1 - pages_meta[::-1].index('…')\n",
    "\n",
    "            button_aria_label = pages[index_of_next_page].find(\"button\")[\"aria-label\"]\n",
    "\n",
    "            # 해당 버튼이 나올때까지 기다려주기\n",
    "\n",
    "            self.driver.implicitly_wait(10)\n",
    "\n",
    "            try:\n",
    "                self.driver.find_element_by_xpath('//*[@aria-label=\"{}\"]'.format(button_aria_label)).click()\n",
    "\n",
    "            except:\n",
    "                self.driver.get(self.driver.current_url)\n",
    "                self.driver.implicitly_wait(10)\n",
    "                button_aria_label = str(int(button_aria_label.split()[0]) + 1) + \" \" + button_aria_label.split()[1]\n",
    "                self.driver.find_element_by_xpath('//*[@aria-label=\"{}\"]'.format(button_aria_label)).click()\n",
    "\n",
    "            self.driver.implicitly_wait(10)\n",
    "            print(\"Upcoming page is {}\".format(i + 2))\n",
    "            upcoming = self.driver.current_url\n",
    "            # Refresh List\n",
    "            try:\n",
    "                pages = self.refresh_source_pages()[0]\n",
    "            except:\n",
    "                self.driver.get(self.driver.current_url)\n",
    "                time.sleep(3)\n",
    "                pages = self.refresh_source_pages()[0]\n",
    "\n",
    "        return (df, upcoming)\n",
    "\n",
    "    def crawl(self, keyword, counts, how_many=3):\n",
    "        header = \"https://www.linkedin.com/jobs/search/?geoId=105149562&keywords=\"\n",
    "        link = header + self.refine(keyword)\n",
    "        self.driver.get(link)\n",
    "\n",
    "        time.sleep(2)\n",
    "        # 밑에 self함수 불러와서 크롤\n",
    "        html = self.driver.page_source\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        self.driver.implicitly_wait(10)\n",
    "\n",
    "        pages = soup.find(\"ul\", {\"class\": \"artdeco-pagination__pages artdeco-pagination__pages--number\"}).find_all(\"li\")\n",
    "\n",
    "        total_page = int(pages[-1].text.strip())\n",
    "        start_url = self.driver.current_url\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        count = df.shape[0]\n",
    "        for i in trange(total_page // how_many):\n",
    "            while count < counts - 1:\n",
    "\n",
    "                starting_page_num = 1 + (3 * i)\n",
    "\n",
    "                try:\n",
    "                    out = self.crawl_job_description(starting_page_num, how_many, total_page, start_url)\n",
    "\n",
    "                except:\n",
    "                    if self.check_http_error() == True:\n",
    "\n",
    "                        if starting_page_num != 1:\n",
    "                            start_url = out[1]\n",
    "\n",
    "                        print(\"Take a break...for 3 minuits...\")\n",
    "                        time.sleep(180)\n",
    "                        self.refrech_link(start_url)\n",
    "                        out = self.crawl_job_description(starting_page_num, how_many, total_page, start_url)\n",
    "                    else:\n",
    "                        print(\"refresh due to error...\")\n",
    "                        self.refresh_link(start_url)\n",
    "                        self.driver.implicitly_wait(10)\n",
    "                        out = self.crawl_job_description(starting_page_num, how_many, total_page, start_url)\n",
    "                start_url = out[1]\n",
    "                df = pd.concat([df, out[0]])\n",
    "                count = df.shape[0]\n",
    "                print(\"Count: {}\".format(count))\n",
    "                print(\"Refreshing for {} times\".format(i + 1))\n",
    "\n",
    "                if count != counts - 1:\n",
    "                    self.refresh_link(start_url)\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 아래부터는 메인 프로그램에 들어가야 할 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/HongSukhyun/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: use options instead of chrome_options\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "driverpath = os.getcwd()+\"/chromedriver\"\n",
    "options = webdriver.ChromeOptions()\n",
    "driver =  webdriver.Chrome(driverpath,  chrome_options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "크롤러 초기 설정중...\n",
      "ID:sukhyun9673@gmail.com\n",
      "PASS: sh96699669\n",
      "안녕하세요 홍석현님, 당신의 직무, 커리어 관심도에 따른 수업 추천을 해드리겠습니다.\n",
      "1. 먼저 관심있는 기업 / 직무 / 산업분야를 질문에 따라 순서대로 입력하세요. '무관' 한 분야는 스킵하셔도 됩니다.\n",
      "2. 이후 입력한 내용에 따라 10개의 Job Description 이 노출됩니다. 이중 마음에 드는 3개를 고르세요!\n",
      "3. 이 결과를 바탕으로 수업을 추천해드립니다\n",
      "**모든 검색어는 영문으로 입력하셔야 합니다!**\n",
      "초기 토픽 갯수 설정 (25, 40, 50중 입력): 40\n",
      "관심있는 산업을 입력해주세요 (없으면 Enter 입력)IT\n",
      "관심있는 기업을 입력해주세요 (없으면 Enter 입력)aws\n",
      "관심있는 직무 입력해주세요(없으면 Enter 입력)sales\n"
     ]
    }
   ],
   "source": [
    "recommender = JDcrawler_recommender(\"홍석현\", driverpath, driver, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = recommender.load_processed()\n",
    "data[\"tokenized\"] = data[\"tokenized\"].apply(literal_eval)\n",
    "keyword = recommender.keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender.login_linkedin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cb599eda1f64011bbae6d6db0c021af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c95b5b5be474f1ab7ae785594e18521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling 1 out of 3 pages...\n",
      "Upcoming page is 2\n",
      "Crawling 2 out of 3 pages...\n",
      "Upcoming page is 3\n",
      "Crawling 3 out of 3 pages...\n",
      "\n",
      "refresh due to error...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/HongSukhyun/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:94: DeprecationWarning: use options instead of chrome_options\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d1d27c73ed54f14a0a8e86844cc9fa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling 1 out of 3 pages...\n",
      "Upcoming page is 2\n",
      "Crawling 2 out of 3 pages...\n",
      "Upcoming page is 3\n",
      "Crawling 3 out of 3 pages...\n",
      "\n",
      "Count: 39\n",
      "Refreshing for 1 times\n",
      "Take a break...for 3 minuits...\n"
     ]
    }
   ],
   "source": [
    "result = recommender.crawl(keyword, counts = 5, how_many=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stopword_filter import stem_word, filter_stopwords, cleanse, sw, stemmer, cleanse_df, tfidf, filter_more\n",
    "\n",
    "result[result.columns[0]] = result[result.columns[0]].apply(lambda x: cleanse(x, stem_words = False))\n",
    "result[result.columns[1]] = result[result.columns[1]].apply(lambda x: cleanse(x, stem_words = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = input(\"Choose index of first JD you are interested: \")\n",
    "second = input(\"Choose index of second JD you are interested: \")\n",
    "third = input(\"Choose index of third JD you are interested: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = result.iloc[[int(first), int(second), int(third)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_doc = target[\"Job_Details\"].values.sum().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_filter(target):\n",
    "    target = \" \".join([i.lower() for i in target])\n",
    "    target = filter_stopwords(target, sw)\n",
    "    target = cleanse(target)\n",
    "    target = target.split()\n",
    "\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_doc = process_filter(target_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LDAModel import train_lda, jsd\n",
    "dictionary,corpus,lda = train_lda(data, num_topics = int(recommender.topicnum), passes = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"내용을 기반으로 추천을 생성하는 중입니다...1~2분정도 소요될 수 있습니다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_dist = np.array([[tup[1] for tup in lst] for lst in lda[corpus]])\n",
    "query_bow = dictionary.doc2bow(target_doc)\n",
    "new_doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=query_bow)])\n",
    "index_to_score = jsd(new_doc_distribution, doc_topic_dist)\n",
    "data[\"distance_with_JD\"] = index_to_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = data[[\"Course_Name\", \"tokenized\", \"div\", \"distance_with_JD\"]].sort_values(by = \"distance_with_JD\")[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
